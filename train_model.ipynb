{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ddd70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy yfinance scikit-learn matplotlib seaborn plotly flask python-dotenv tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# # IT Companies\n",
    "# IT_companies = [\n",
    "#     'TCS.NS',\n",
    "#     'INFY.NS',\n",
    "#     'LTIM.NS',\n",
    "#     'TECHM.NS',\n",
    "#     'WIPRO.NS',\n",
    "#     'ACN',\n",
    "#     'CTSH',\n",
    "#     'BSOFT.NS'\n",
    "# ]\n",
    "\n",
    "# # Non-IT Companies\n",
    "# NonIT_companies = [\n",
    "#     'HDFCBANK.NS',\n",
    "#     'G',\n",
    "#     'LT.NS',\n",
    "#     'RELIANCE.NS',\n",
    "#     'TATASTEEL.NS'   # Replaced TATAMOTORS (more stable)\n",
    "# ]\n",
    "\n",
    "COMPANIES = {\n",
    "    # -------- IT --------\n",
    "    \"TCS\": \"TCS.NS\",\n",
    "    \"Infosys\": \"INFY.NS\",\n",
    "    \"LTIMindtree\": \"LTIM.NS\",\n",
    "    \"Tech Mahindra\": \"TECHM.NS\",\n",
    "    \"Wipro\": \"WIPRO.NS\",\n",
    "    \"HCL Tech\": \"HCLTECH.NS\",\n",
    "    \"Accenture\": \"ACN\",\n",
    "    \"Cognizant\": \"CTSH\",\n",
    "    \"Birlasoft\": \"BSOFT.NS\",\n",
    "\n",
    "    # -------- Non-IT --------\n",
    "    \"Sagility\": \"SAGILITY.NS\",\n",
    "    \"Genpact\": \"G\",\n",
    "    \"L&T\": \"LT.NS\",\n",
    "    \"Reliance\": \"RELIANCE.NS\",\n",
    "    \"Tata Consumer\": \"TATACONSUM.NS\"\n",
    "}\n",
    "\n",
    "\n",
    "all_stocks = list(COMPANIES.values())\n",
    "\n",
    "# Download data safely\n",
    "all_data = {}\n",
    "\n",
    "for ticker in all_stocks:\n",
    "    print(f\"Downloading {ticker}...\")\n",
    "    try:\n",
    "        df = yf.download(ticker, period=\"5y\", \n",
    "            auto_adjust=True,   # important\n",
    "            threads=False,  # Better than manual date\n",
    "            progress=False\n",
    "        )\n",
    "\n",
    "        if not df.empty:\n",
    "            all_data[ticker] = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "        else:\n",
    "            print(f\"{ticker} returned empty data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {ticker}: {e}\")\n",
    "\n",
    "# Combine\n",
    "data = pd.concat(all_data, axis=1)\n",
    "\n",
    "# Flatten columns\n",
    "data.columns = [f\"{col[0]}_{col[1]}\" for col in data.columns]\n",
    "\n",
    "# Save CSV\n",
    "output_folder = r'C:\\Users\\Sai\\OneDrive\\Documents\\uma_project'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(output_folder, 'stock_price_5yrs.csv')\n",
    "data.to_csv(file_path)\n",
    "\n",
    "print(\"\\nData saved successfully!\")\n",
    "print(\"Start Date:\", data.index.min())\n",
    "print(\"End Date:\", data.index.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d82bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "df = pd.read_csv(\"stock_price_5yrs.csv\")\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4762a692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1298, 61)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ff2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabc0a4",
   "metadata": {},
   "source": [
    "<h3>code for handling null values, new features, create targets/features<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceda191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"stock_price_5yrs.csv\", parse_dates=['Date'])\n",
    "df = df.sort_values('Date').set_index('Date')\n",
    "\n",
    "# Separate columns\n",
    "price_cols = [c for c in df.columns if any(x in c for x in ['Open','High','Low','Close']) and 'Volume' not in c]\n",
    "volume_cols = [c for c in df.columns if 'Volume' in c]\n",
    "\n",
    "# 1️⃣ Handle nulls\n",
    "df[price_cols] = df[price_cols].ffill().bfill()\n",
    "df[volume_cols] = df[volume_cols].ffill().bfill().fillna(0)\n",
    "\n",
    "# 2️⃣ Create features in separate DataFrames to avoid fragmentation\n",
    "feature_dfs = []\n",
    "\n",
    "for col in price_cols:\n",
    "    temp = pd.DataFrame({\n",
    "        f'{col}_MA20': df[col].rolling(20).mean(),\n",
    "        f'{col}_MA50': df[col].rolling(50).mean(),\n",
    "        f'{col}_Return': df[col].pct_change() * 100,\n",
    "        f'{col}_Volatility': df[col].rolling(20).std(),\n",
    "        f'{col}_Next': df[col].shift(-1) if 'Close' in col else None  # Next-day Close only for Close columns\n",
    "    }, index=df.index)\n",
    "    \n",
    "    # Remove None columns\n",
    "    temp = temp[[c for c in temp.columns if temp[c].notna().any()]]\n",
    "    \n",
    "    feature_dfs.append(temp)\n",
    "\n",
    "# Concatenate all features with original df\n",
    "df = pd.concat([df, *feature_dfs], axis=1)\n",
    "\n",
    "# 3️⃣ Drop rows with NaNs created by rolling / shift\n",
    "df = df.dropna()\n",
    "\n",
    "# 4️⃣ Save final dataset\n",
    "df.to_csv(\"stock_features_ready.csv\")\n",
    "\n",
    "print(\"Dataset ready! Columns now include features and targets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662e1a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1248, 264)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "007836f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All feature columns (exclude Volume if you like)\n",
    "feature_cols = [c for c in df.columns if any(x in c for x in ['MA20','MA50','Return','Volatility'])]\n",
    "target_cols = [c for c in df.columns if '_Next' in c]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "timesteps = 60\n",
    "\n",
    "results = []\n",
    "\n",
    "company_models = {}\n",
    "scalers_X = {}\n",
    "scalers_y = {}\n",
    "company_features_dict = {}\n",
    "\n",
    "for target in target_cols:\n",
    "\n",
    "    print(f\"\\nTraining model for: {target}\")\n",
    "\n",
    "    # Select features only related to this company\n",
    "    company_prefix = target.replace(\"_Close_Next\", \"\")\n",
    "    company_features = [\n",
    "        c for c in feature_cols \n",
    "        if company_prefix in c and c != target\n",
    "    ]\n",
    "    \n",
    "    # Save features for prediction\n",
    "    company_features_dict[target] = company_features\n",
    "\n",
    "    X = df[company_features]\n",
    "    y = df[[target]]\n",
    "\n",
    "    # --- Create Sequences ---\n",
    "    X_seq, y_seq = [], []\n",
    "\n",
    "    for i in range(timesteps, len(df)):\n",
    "        X_seq.append(X.iloc[i-timesteps:i].values)\n",
    "        y_seq.append(y.iloc[i].values)\n",
    "\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "\n",
    "    # --- Scaling (separate per company) ---\n",
    "    # --- Train/Test Split (BEFORE scaling) ---\n",
    "    train_size = int(len(X_seq) * 0.8)\n",
    "\n",
    "    X_train = X_seq[:train_size]\n",
    "    X_test = X_seq[train_size:]\n",
    "\n",
    "    y_train = y_seq[:train_size]\n",
    "    y_test = y_seq[train_size:]\n",
    "\n",
    "    # --- Scaling (fit ONLY on train) ---\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "    nsamples, ntimesteps, nfeatures = X_train.shape\n",
    "\n",
    "    # Flatten only TRAIN\n",
    "    X_train_flat = X_train.reshape((nsamples * ntimesteps, nfeatures))\n",
    "    scaler_X.fit(X_train_flat)\n",
    "\n",
    "    # Transform train\n",
    "    X_train_scaled = scaler_X.transform(X_train_flat).reshape(X_train.shape)\n",
    "\n",
    "    # Transform test (do NOT fit again)\n",
    "    X_test_flat = X_test.reshape((X_test.shape[0] * ntimesteps, nfeatures))\n",
    "    X_test_scaled = scaler_X.transform(X_test_flat).reshape(X_test.shape)\n",
    "\n",
    "    # Scale y\n",
    "    scaler_y.fit(y_train.reshape(-1,1))\n",
    "\n",
    "    y_train_scaled = scaler_y.transform(y_train.reshape(-1,1))\n",
    "    y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
    "\n",
    "\n",
    "    # --- Model ---\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(126, return_sequences=True, input_shape=(timesteps, nfeatures)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                                patience=10,\n",
    "                                restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train_scaled,\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    y_pred_scaled = model.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    y_actual = y_test\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_pred))\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    mape = np.mean(np.abs((y_actual - y_pred) / (y_actual + 1e-8))) * 100\n",
    "\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Flatten for Linear Regression\n",
    "    X_flat_train = X_train_scaled.reshape(X_train_scaled.shape[0], -1)\n",
    "    X_flat_test = X_test_scaled.reshape(X_test_scaled.shape[0], -1)\n",
    "\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_flat_train, y_train)\n",
    "\n",
    "    lr_pred = lr_model.predict(X_flat_test)\n",
    "\n",
    "    lr_rmse = np.sqrt(mean_squared_error(y_actual, lr_pred))\n",
    "    lr_mae = mean_absolute_error(y_actual, lr_pred)\n",
    "    lr_mape = np.mean(np.abs((y_actual - lr_pred) / (y_actual + 1e-8))) * 100\n",
    "\n",
    "    results.append([\n",
    "        target,\n",
    "        rmse, mae, mape,\n",
    "        lr_rmse, lr_mae, lr_mape\n",
    "    ])\n",
    "\n",
    "    # Save model and scalers for this company\n",
    "    company_models[target] = model\n",
    "    scalers_X[target] = scaler_X\n",
    "    scalers_y[target] = scaler_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6aed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Evaluate Model ---\n",
    "evaluation_df = pd.DataFrame(results,\n",
    "                             columns=[\n",
    "                                 \"Company\",\n",
    "                                 \"LSTM_RMSE\",\n",
    "                                 \"LSTM_MAE\",\n",
    "                                 \"LSTM_MAPE(%)\",\n",
    "                                 \"LR_RMSE\",\n",
    "                                 \"LR_MAE\",\n",
    "                                 \"LR_MAPE(%)\"\n",
    "                             ])\n",
    "\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "print(evaluation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "companies = [c.replace('_Next','') for c in target_cols]\n",
    "\n",
    "for target in target_cols:\n",
    "    comp = target.replace('_Next','')\n",
    "    print(f\"\\nPredicting 7-day future for: {comp}\")\n",
    "\n",
    "    model = company_models[target]\n",
    "    scaler_X = scalers_X[target]\n",
    "    scaler_y = scalers_y[target]\n",
    "\n",
    "    # Features for this company\n",
    "    company_features = company_features_dict[target]\n",
    "    X = df[company_features].values  # full feature data\n",
    "\n",
    "    # Last 60 days\n",
    "    last_seq = X[-timesteps:].copy()\n",
    "    last_seq_scaled = scaler_X.transform(last_seq).reshape(1, timesteps, len(company_features))\n",
    "\n",
    "    future_preds = []\n",
    "    for _ in range(7):\n",
    "        next_pred_scaled = model.predict(last_seq_scaled, verbose=0)\n",
    "        next_pred = scaler_y.inverse_transform(next_pred_scaled)[0, 0]\n",
    "        future_preds.append(next_pred)\n",
    "\n",
    "        # Update last_seq_scaled\n",
    "        new_features = last_seq_scaled[0, -1, :].copy()\n",
    "        new_features[-1] = next_pred_scaled.item()\n",
    "        last_seq_scaled = np.roll(last_seq_scaled, -1, axis=1)\n",
    "        last_seq_scaled[0, -1, :] = new_features\n",
    "\n",
    "    # Dates\n",
    "    actual_prices = df[comp].values[-30:]\n",
    "    last_date = df.index[-1]\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=7)\n",
    "\n",
    "    # Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Actual Prices\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df.index[-30:], \n",
    "        y=actual_prices,\n",
    "        mode='lines+markers',\n",
    "        name='Actual Price',\n",
    "        fill='tozeroy',\n",
    "        hovertemplate='<b>Actual</b><br>Date: %{x}<br>Price: ₹%{y:,.2f}<extra></extra>'  # Custom tooltip\n",
    "\n",
    "    ))\n",
    "\n",
    "    # Predicted Prices\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=future_dates,\n",
    "        y=future_preds,\n",
    "        mode='lines+markers',\n",
    "        name='Predicted Price',\n",
    "        line=dict(dash='dash', color='red'),\n",
    "        hovertemplate='<b>Predicted</b><br>Date: %{x}<br>Price: ₹%{y:,.2f}<extra></extra>'  # Custom tooltip\n",
    "\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"{comp} Close Price: Actual vs Predicted (7-day forecast)\",\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Price',\n",
    "        legend=dict(x=1,      # Right edge (0=left, 1=right)\n",
    "                    y=1,      # Top edge (0=bottom, 1=top)\n",
    "                    # xanchor='right',  # Anchor legend's right side to x position\n",
    "                    # yanchor='top' # Anchor legend's top to y position\n",
    "                ),\n",
    "        template='plotly_white',\n",
    "        width=500,   # set width in pixels\n",
    "        height=300 ,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd60c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save each model separately\n",
    "for name, model in company_models.items():\n",
    "    model.save(f\"models/{name}.keras\")\n",
    "\n",
    "# Save scalers & feature dict\n",
    "joblib.dump(scalers_X, \"scalers_X.pkl\")\n",
    "joblib.dump(scalers_y, \"scalers_y.pkl\")\n",
    "joblib.dump(company_features_dict, \"features.pkl\")\n",
    "\n",
    "print(\"Models and scalers saved properly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7968ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
